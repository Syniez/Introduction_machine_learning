{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIcR-W9xC4c8"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3LIueafDMIS"
      },
      "source": [
        "# Functions\n",
        "\n",
        "def sigmoid(s):\n",
        "  return 1 / (1 + np.exp(-s))\n",
        "\n",
        "def loss(y, h):\n",
        "  return (-y * np.log(h) - (1-y) * np.log(1-h)).sum()\n",
        "  \n",
        "def gradient(X, y, w):\n",
        "  return -(y * X) / (1 + np.exp(y * np.dot(X, w)))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9CICraODa5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb30b90-952a-4839-bb01-5ffc89e102aa"
      },
      "source": [
        "# Gradient Descent Algorithm\n",
        "\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_classes=2, n_clusters_per_class=1, n_informative=2, n_redundant=0, random_state=2021)\n",
        "\n",
        "X_train = np.append(np.ones((X.shape[0], 1)), X, axis=1)              # add bias  (200, 3)\n",
        "y_train = np.array([[1] if label == 1 else [-1] for label in y])      # labeling  (200, 1)\n",
        "y_loss = np.array([[1] if label == 1 else [0] for label in y])        # labeling\n",
        "w = np.array([[np.random.uniform(-1, 1)] for _ in range(X_train.shape[1])])   # weight initialize (3, 1)\n",
        "\n",
        "max_iter = 200\n",
        "learning_rate = 0.1\n",
        "threshold = 0.5\n",
        "\n",
        "for _ in range(max_iter):\n",
        "  v = -np.mean(gradient(X_train, y_train, w)[:, :], axis=0)\n",
        "  v = v[:, np.newaxis]\n",
        "  w += learning_rate * v\n",
        "  \n",
        "  probs = sigmoid(np.dot(X_train, w))   # prediction\n",
        "  y_hat = [[1] if p > threshold else [-1] for p in probs]\n",
        "\n",
        "  print(\"loss : {:.2f}, accuracy : {:.2f}\".format(loss(y_loss, probs), accuracy_score(y_train, y_hat)))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss : 274.68, accuracy : 0.30\n",
            "loss : 263.04, accuracy : 0.35\n",
            "loss : 251.77, accuracy : 0.35\n",
            "loss : 240.90, accuracy : 0.37\n",
            "loss : 230.45, accuracy : 0.37\n",
            "loss : 220.41, accuracy : 0.39\n",
            "loss : 210.81, accuracy : 0.42\n",
            "loss : 201.67, accuracy : 0.46\n",
            "loss : 192.98, accuracy : 0.48\n",
            "loss : 184.76, accuracy : 0.49\n",
            "loss : 177.02, accuracy : 0.52\n",
            "loss : 169.75, accuracy : 0.52\n",
            "loss : 162.95, accuracy : 0.52\n",
            "loss : 156.61, accuracy : 0.51\n",
            "loss : 150.72, accuracy : 0.50\n",
            "loss : 145.27, accuracy : 0.52\n",
            "loss : 140.23, accuracy : 0.53\n",
            "loss : 135.57, accuracy : 0.57\n",
            "loss : 131.28, accuracy : 0.59\n",
            "loss : 127.33, accuracy : 0.65\n",
            "loss : 123.69, accuracy : 0.67\n",
            "loss : 120.34, accuracy : 0.70\n",
            "loss : 117.24, accuracy : 0.72\n",
            "loss : 114.39, accuracy : 0.76\n",
            "loss : 111.75, accuracy : 0.78\n",
            "loss : 109.31, accuracy : 0.81\n",
            "loss : 107.05, accuracy : 0.81\n",
            "loss : 104.95, accuracy : 0.83\n",
            "loss : 103.00, accuracy : 0.84\n",
            "loss : 101.17, accuracy : 0.84\n",
            "loss : 99.47, accuracy : 0.84\n",
            "loss : 97.88, accuracy : 0.85\n",
            "loss : 96.39, accuracy : 0.85\n",
            "loss : 94.99, accuracy : 0.86\n",
            "loss : 93.67, accuracy : 0.86\n",
            "loss : 92.42, accuracy : 0.86\n",
            "loss : 91.25, accuracy : 0.87\n",
            "loss : 90.14, accuracy : 0.87\n",
            "loss : 89.09, accuracy : 0.88\n",
            "loss : 88.10, accuracy : 0.89\n",
            "loss : 87.15, accuracy : 0.89\n",
            "loss : 86.25, accuracy : 0.89\n",
            "loss : 85.39, accuracy : 0.89\n",
            "loss : 84.58, accuracy : 0.89\n",
            "loss : 83.80, accuracy : 0.89\n",
            "loss : 83.05, accuracy : 0.89\n",
            "loss : 82.34, accuracy : 0.89\n",
            "loss : 81.65, accuracy : 0.89\n",
            "loss : 81.00, accuracy : 0.89\n",
            "loss : 80.37, accuracy : 0.89\n",
            "loss : 79.77, accuracy : 0.89\n",
            "loss : 79.19, accuracy : 0.90\n",
            "loss : 78.63, accuracy : 0.90\n",
            "loss : 78.09, accuracy : 0.90\n",
            "loss : 77.57, accuracy : 0.90\n",
            "loss : 77.07, accuracy : 0.90\n",
            "loss : 76.59, accuracy : 0.90\n",
            "loss : 76.13, accuracy : 0.91\n",
            "loss : 75.68, accuracy : 0.91\n",
            "loss : 75.24, accuracy : 0.91\n",
            "loss : 74.82, accuracy : 0.91\n",
            "loss : 74.41, accuracy : 0.90\n",
            "loss : 74.02, accuracy : 0.90\n",
            "loss : 73.63, accuracy : 0.90\n",
            "loss : 73.26, accuracy : 0.90\n",
            "loss : 72.90, accuracy : 0.90\n",
            "loss : 72.55, accuracy : 0.90\n",
            "loss : 72.21, accuracy : 0.90\n",
            "loss : 71.88, accuracy : 0.90\n",
            "loss : 71.56, accuracy : 0.90\n",
            "loss : 71.25, accuracy : 0.90\n",
            "loss : 70.95, accuracy : 0.90\n",
            "loss : 70.65, accuracy : 0.90\n",
            "loss : 70.36, accuracy : 0.90\n",
            "loss : 70.08, accuracy : 0.90\n",
            "loss : 69.81, accuracy : 0.90\n",
            "loss : 69.54, accuracy : 0.90\n",
            "loss : 69.28, accuracy : 0.90\n",
            "loss : 69.03, accuracy : 0.90\n",
            "loss : 68.78, accuracy : 0.90\n",
            "loss : 68.54, accuracy : 0.90\n",
            "loss : 68.30, accuracy : 0.90\n",
            "loss : 68.07, accuracy : 0.90\n",
            "loss : 67.84, accuracy : 0.90\n",
            "loss : 67.62, accuracy : 0.90\n",
            "loss : 67.41, accuracy : 0.90\n",
            "loss : 67.20, accuracy : 0.90\n",
            "loss : 66.99, accuracy : 0.90\n",
            "loss : 66.79, accuracy : 0.90\n",
            "loss : 66.59, accuracy : 0.90\n",
            "loss : 66.40, accuracy : 0.90\n",
            "loss : 66.21, accuracy : 0.90\n",
            "loss : 66.02, accuracy : 0.90\n",
            "loss : 65.84, accuracy : 0.90\n",
            "loss : 65.66, accuracy : 0.90\n",
            "loss : 65.49, accuracy : 0.90\n",
            "loss : 65.32, accuracy : 0.90\n",
            "loss : 65.15, accuracy : 0.90\n",
            "loss : 64.98, accuracy : 0.90\n",
            "loss : 64.82, accuracy : 0.90\n",
            "loss : 64.66, accuracy : 0.90\n",
            "loss : 64.51, accuracy : 0.90\n",
            "loss : 64.36, accuracy : 0.90\n",
            "loss : 64.21, accuracy : 0.90\n",
            "loss : 64.06, accuracy : 0.90\n",
            "loss : 63.91, accuracy : 0.90\n",
            "loss : 63.77, accuracy : 0.90\n",
            "loss : 63.63, accuracy : 0.90\n",
            "loss : 63.49, accuracy : 0.90\n",
            "loss : 63.36, accuracy : 0.90\n",
            "loss : 63.23, accuracy : 0.90\n",
            "loss : 63.10, accuracy : 0.90\n",
            "loss : 62.97, accuracy : 0.90\n",
            "loss : 62.84, accuracy : 0.90\n",
            "loss : 62.72, accuracy : 0.90\n",
            "loss : 62.60, accuracy : 0.90\n",
            "loss : 62.48, accuracy : 0.90\n",
            "loss : 62.36, accuracy : 0.90\n",
            "loss : 62.24, accuracy : 0.90\n",
            "loss : 62.13, accuracy : 0.90\n",
            "loss : 62.01, accuracy : 0.90\n",
            "loss : 61.90, accuracy : 0.90\n",
            "loss : 61.79, accuracy : 0.90\n",
            "loss : 61.68, accuracy : 0.90\n",
            "loss : 61.58, accuracy : 0.90\n",
            "loss : 61.47, accuracy : 0.90\n",
            "loss : 61.37, accuracy : 0.90\n",
            "loss : 61.27, accuracy : 0.90\n",
            "loss : 61.17, accuracy : 0.90\n",
            "loss : 61.07, accuracy : 0.90\n",
            "loss : 60.97, accuracy : 0.90\n",
            "loss : 60.88, accuracy : 0.90\n",
            "loss : 60.78, accuracy : 0.90\n",
            "loss : 60.69, accuracy : 0.90\n",
            "loss : 60.60, accuracy : 0.90\n",
            "loss : 60.51, accuracy : 0.90\n",
            "loss : 60.42, accuracy : 0.90\n",
            "loss : 60.33, accuracy : 0.90\n",
            "loss : 60.24, accuracy : 0.90\n",
            "loss : 60.16, accuracy : 0.90\n",
            "loss : 60.07, accuracy : 0.90\n",
            "loss : 59.99, accuracy : 0.90\n",
            "loss : 59.91, accuracy : 0.90\n",
            "loss : 59.83, accuracy : 0.90\n",
            "loss : 59.75, accuracy : 0.90\n",
            "loss : 59.67, accuracy : 0.90\n",
            "loss : 59.59, accuracy : 0.90\n",
            "loss : 59.51, accuracy : 0.90\n",
            "loss : 59.43, accuracy : 0.90\n",
            "loss : 59.36, accuracy : 0.90\n",
            "loss : 59.28, accuracy : 0.90\n",
            "loss : 59.21, accuracy : 0.90\n",
            "loss : 59.14, accuracy : 0.90\n",
            "loss : 59.07, accuracy : 0.90\n",
            "loss : 59.00, accuracy : 0.90\n",
            "loss : 58.93, accuracy : 0.90\n",
            "loss : 58.86, accuracy : 0.90\n",
            "loss : 58.79, accuracy : 0.90\n",
            "loss : 58.72, accuracy : 0.90\n",
            "loss : 58.65, accuracy : 0.90\n",
            "loss : 58.59, accuracy : 0.90\n",
            "loss : 58.52, accuracy : 0.90\n",
            "loss : 58.46, accuracy : 0.90\n",
            "loss : 58.39, accuracy : 0.90\n",
            "loss : 58.33, accuracy : 0.90\n",
            "loss : 58.27, accuracy : 0.90\n",
            "loss : 58.21, accuracy : 0.90\n",
            "loss : 58.15, accuracy : 0.90\n",
            "loss : 58.09, accuracy : 0.90\n",
            "loss : 58.03, accuracy : 0.90\n",
            "loss : 57.97, accuracy : 0.90\n",
            "loss : 57.91, accuracy : 0.90\n",
            "loss : 57.85, accuracy : 0.90\n",
            "loss : 57.79, accuracy : 0.90\n",
            "loss : 57.74, accuracy : 0.90\n",
            "loss : 57.68, accuracy : 0.90\n",
            "loss : 57.63, accuracy : 0.90\n",
            "loss : 57.57, accuracy : 0.90\n",
            "loss : 57.52, accuracy : 0.90\n",
            "loss : 57.46, accuracy : 0.90\n",
            "loss : 57.41, accuracy : 0.90\n",
            "loss : 57.36, accuracy : 0.90\n",
            "loss : 57.31, accuracy : 0.90\n",
            "loss : 57.26, accuracy : 0.90\n",
            "loss : 57.21, accuracy : 0.90\n",
            "loss : 57.16, accuracy : 0.90\n",
            "loss : 57.11, accuracy : 0.90\n",
            "loss : 57.06, accuracy : 0.90\n",
            "loss : 57.01, accuracy : 0.90\n",
            "loss : 56.96, accuracy : 0.90\n",
            "loss : 56.91, accuracy : 0.90\n",
            "loss : 56.86, accuracy : 0.90\n",
            "loss : 56.82, accuracy : 0.90\n",
            "loss : 56.77, accuracy : 0.90\n",
            "loss : 56.72, accuracy : 0.90\n",
            "loss : 56.68, accuracy : 0.90\n",
            "loss : 56.63, accuracy : 0.90\n",
            "loss : 56.59, accuracy : 0.90\n",
            "loss : 56.55, accuracy : 0.90\n",
            "loss : 56.50, accuracy : 0.90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRIlhXpAET7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50d0261-2a62-45b8-bd4b-98499af5ac10"
      },
      "source": [
        "# Stochastic Gradient Descent Algorithm\n",
        "\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_classes=2, n_clusters_per_class=1, n_informative=2, n_redundant=0, random_state=2021)\n",
        "\n",
        "X_train = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
        "y_train = np.array([[1] if label == 1 else [-1] for label in y])\n",
        "y_loss = np.array([[1] if label == 1 else [0] for label in y])\n",
        "w = np.array([[np.random.uniform(-1, 1)] for _ in range(X_train.shape[1])])\n",
        "\n",
        "max_iter = 200\n",
        "learning_rate = 0.1\n",
        "threshold = 0.5\n",
        "\n",
        "for _ in range(max_iter):\n",
        "  ind = np.squeeze(np.random.choice(200, 1, replace=False))\n",
        "  v = -gradient(X_train[ind], y_train[ind], w)\n",
        "  v = v[:, np.newaxis]\n",
        "  w += learning_rate * v\n",
        "  \n",
        "  probs = sigmoid(np.dot(X_train, w))\n",
        "  y_hat = [[1] if p > threshold else [-1] for p in probs]\n",
        "\n",
        "  print(\"loss : {:.2f}, accuracy : {:.2f}\".format(loss(y_loss, probs), accuracy_score(y_train, y_hat)))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss : 258.26, accuracy : 0.28\n",
            "loss : 245.64, accuracy : 0.25\n",
            "loss : 210.03, accuracy : 0.27\n",
            "loss : 188.58, accuracy : 0.32\n",
            "loss : 170.62, accuracy : 0.41\n",
            "loss : 171.57, accuracy : 0.41\n",
            "loss : 166.02, accuracy : 0.43\n",
            "loss : 165.79, accuracy : 0.44\n",
            "loss : 163.19, accuracy : 0.46\n",
            "loss : 162.16, accuracy : 0.46\n",
            "loss : 162.59, accuracy : 0.48\n",
            "loss : 142.11, accuracy : 0.57\n",
            "loss : 142.29, accuracy : 0.57\n",
            "loss : 135.85, accuracy : 0.67\n",
            "loss : 132.61, accuracy : 0.67\n",
            "loss : 125.32, accuracy : 0.70\n",
            "loss : 127.26, accuracy : 0.69\n",
            "loss : 127.34, accuracy : 0.70\n",
            "loss : 126.36, accuracy : 0.71\n",
            "loss : 123.61, accuracy : 0.70\n",
            "loss : 120.07, accuracy : 0.72\n",
            "loss : 121.73, accuracy : 0.71\n",
            "loss : 114.71, accuracy : 0.74\n",
            "loss : 113.28, accuracy : 0.72\n",
            "loss : 112.09, accuracy : 0.76\n",
            "loss : 108.44, accuracy : 0.79\n",
            "loss : 109.51, accuracy : 0.78\n",
            "loss : 104.09, accuracy : 0.81\n",
            "loss : 105.47, accuracy : 0.79\n",
            "loss : 103.22, accuracy : 0.81\n",
            "loss : 103.63, accuracy : 0.81\n",
            "loss : 102.60, accuracy : 0.80\n",
            "loss : 97.19, accuracy : 0.83\n",
            "loss : 97.22, accuracy : 0.83\n",
            "loss : 91.16, accuracy : 0.84\n",
            "loss : 87.76, accuracy : 0.82\n",
            "loss : 86.93, accuracy : 0.82\n",
            "loss : 86.25, accuracy : 0.81\n",
            "loss : 84.56, accuracy : 0.83\n",
            "loss : 84.17, accuracy : 0.85\n",
            "loss : 83.58, accuracy : 0.84\n",
            "loss : 81.80, accuracy : 0.85\n",
            "loss : 81.40, accuracy : 0.87\n",
            "loss : 83.01, accuracy : 0.88\n",
            "loss : 85.37, accuracy : 0.86\n",
            "loss : 86.47, accuracy : 0.86\n",
            "loss : 84.74, accuracy : 0.87\n",
            "loss : 83.45, accuracy : 0.86\n",
            "loss : 84.64, accuracy : 0.85\n",
            "loss : 83.83, accuracy : 0.85\n",
            "loss : 81.75, accuracy : 0.86\n",
            "loss : 82.00, accuracy : 0.85\n",
            "loss : 84.11, accuracy : 0.85\n",
            "loss : 88.95, accuracy : 0.81\n",
            "loss : 85.78, accuracy : 0.83\n",
            "loss : 84.08, accuracy : 0.84\n",
            "loss : 81.30, accuracy : 0.84\n",
            "loss : 80.36, accuracy : 0.84\n",
            "loss : 79.72, accuracy : 0.86\n",
            "loss : 78.34, accuracy : 0.86\n",
            "loss : 77.33, accuracy : 0.87\n",
            "loss : 75.91, accuracy : 0.87\n",
            "loss : 74.79, accuracy : 0.87\n",
            "loss : 74.07, accuracy : 0.87\n",
            "loss : 73.68, accuracy : 0.86\n",
            "loss : 73.11, accuracy : 0.87\n",
            "loss : 72.72, accuracy : 0.87\n",
            "loss : 72.43, accuracy : 0.87\n",
            "loss : 73.50, accuracy : 0.86\n",
            "loss : 73.33, accuracy : 0.85\n",
            "loss : 72.72, accuracy : 0.86\n",
            "loss : 72.10, accuracy : 0.86\n",
            "loss : 71.60, accuracy : 0.87\n",
            "loss : 71.12, accuracy : 0.87\n",
            "loss : 71.02, accuracy : 0.86\n",
            "loss : 70.51, accuracy : 0.87\n",
            "loss : 69.76, accuracy : 0.88\n",
            "loss : 69.28, accuracy : 0.88\n",
            "loss : 69.37, accuracy : 0.87\n",
            "loss : 70.65, accuracy : 0.85\n",
            "loss : 70.47, accuracy : 0.85\n",
            "loss : 70.40, accuracy : 0.86\n",
            "loss : 70.53, accuracy : 0.86\n",
            "loss : 70.49, accuracy : 0.86\n",
            "loss : 70.25, accuracy : 0.86\n",
            "loss : 70.64, accuracy : 0.85\n",
            "loss : 70.34, accuracy : 0.85\n",
            "loss : 69.19, accuracy : 0.86\n",
            "loss : 69.41, accuracy : 0.86\n",
            "loss : 70.45, accuracy : 0.85\n",
            "loss : 70.51, accuracy : 0.85\n",
            "loss : 70.45, accuracy : 0.85\n",
            "loss : 69.55, accuracy : 0.85\n",
            "loss : 68.19, accuracy : 0.86\n",
            "loss : 68.12, accuracy : 0.86\n",
            "loss : 67.30, accuracy : 0.86\n",
            "loss : 66.28, accuracy : 0.88\n",
            "loss : 66.14, accuracy : 0.87\n",
            "loss : 65.76, accuracy : 0.88\n",
            "loss : 65.81, accuracy : 0.88\n",
            "loss : 65.52, accuracy : 0.87\n",
            "loss : 65.40, accuracy : 0.87\n",
            "loss : 64.41, accuracy : 0.88\n",
            "loss : 64.65, accuracy : 0.88\n",
            "loss : 64.33, accuracy : 0.88\n",
            "loss : 64.48, accuracy : 0.88\n",
            "loss : 63.49, accuracy : 0.88\n",
            "loss : 63.75, accuracy : 0.88\n",
            "loss : 63.98, accuracy : 0.88\n",
            "loss : 63.06, accuracy : 0.88\n",
            "loss : 62.51, accuracy : 0.88\n",
            "loss : 62.55, accuracy : 0.88\n",
            "loss : 62.52, accuracy : 0.88\n",
            "loss : 62.55, accuracy : 0.88\n",
            "loss : 62.52, accuracy : 0.88\n",
            "loss : 62.71, accuracy : 0.88\n",
            "loss : 62.99, accuracy : 0.88\n",
            "loss : 67.67, accuracy : 0.86\n",
            "loss : 67.77, accuracy : 0.86\n",
            "loss : 73.79, accuracy : 0.82\n",
            "loss : 72.62, accuracy : 0.83\n",
            "loss : 69.79, accuracy : 0.85\n",
            "loss : 69.50, accuracy : 0.85\n",
            "loss : 70.01, accuracy : 0.84\n",
            "loss : 68.91, accuracy : 0.85\n",
            "loss : 67.66, accuracy : 0.86\n",
            "loss : 67.12, accuracy : 0.86\n",
            "loss : 66.30, accuracy : 0.87\n",
            "loss : 66.34, accuracy : 0.86\n",
            "loss : 66.87, accuracy : 0.86\n",
            "loss : 65.36, accuracy : 0.87\n",
            "loss : 65.78, accuracy : 0.87\n",
            "loss : 66.20, accuracy : 0.86\n",
            "loss : 64.49, accuracy : 0.86\n",
            "loss : 64.39, accuracy : 0.87\n",
            "loss : 64.14, accuracy : 0.87\n",
            "loss : 62.83, accuracy : 0.88\n",
            "loss : 62.83, accuracy : 0.88\n",
            "loss : 61.98, accuracy : 0.88\n",
            "loss : 62.03, accuracy : 0.88\n",
            "loss : 61.17, accuracy : 0.89\n",
            "loss : 61.18, accuracy : 0.89\n",
            "loss : 59.90, accuracy : 0.89\n",
            "loss : 59.06, accuracy : 0.89\n",
            "loss : 59.04, accuracy : 0.89\n",
            "loss : 58.70, accuracy : 0.89\n",
            "loss : 58.68, accuracy : 0.89\n",
            "loss : 57.99, accuracy : 0.88\n",
            "loss : 58.18, accuracy : 0.88\n",
            "loss : 58.17, accuracy : 0.88\n",
            "loss : 57.62, accuracy : 0.88\n",
            "loss : 57.03, accuracy : 0.89\n",
            "loss : 56.63, accuracy : 0.90\n",
            "loss : 56.41, accuracy : 0.90\n",
            "loss : 56.36, accuracy : 0.90\n",
            "loss : 57.10, accuracy : 0.90\n",
            "loss : 57.03, accuracy : 0.90\n",
            "loss : 56.88, accuracy : 0.90\n",
            "loss : 56.83, accuracy : 0.90\n",
            "loss : 56.80, accuracy : 0.90\n",
            "loss : 56.75, accuracy : 0.90\n",
            "loss : 56.74, accuracy : 0.90\n",
            "loss : 57.07, accuracy : 0.89\n",
            "loss : 57.04, accuracy : 0.89\n",
            "loss : 56.59, accuracy : 0.90\n",
            "loss : 56.15, accuracy : 0.90\n",
            "loss : 56.06, accuracy : 0.90\n",
            "loss : 56.05, accuracy : 0.90\n",
            "loss : 56.80, accuracy : 0.90\n",
            "loss : 56.74, accuracy : 0.90\n",
            "loss : 56.69, accuracy : 0.90\n",
            "loss : 56.31, accuracy : 0.90\n",
            "loss : 56.30, accuracy : 0.90\n",
            "loss : 56.11, accuracy : 0.90\n",
            "loss : 58.17, accuracy : 0.89\n",
            "loss : 57.69, accuracy : 0.89\n",
            "loss : 57.74, accuracy : 0.89\n",
            "loss : 57.70, accuracy : 0.89\n",
            "loss : 57.24, accuracy : 0.89\n",
            "loss : 57.36, accuracy : 0.89\n",
            "loss : 56.89, accuracy : 0.90\n",
            "loss : 56.84, accuracy : 0.90\n",
            "loss : 56.73, accuracy : 0.90\n",
            "loss : 56.70, accuracy : 0.90\n",
            "loss : 56.29, accuracy : 0.90\n",
            "loss : 56.28, accuracy : 0.90\n",
            "loss : 57.36, accuracy : 0.90\n",
            "loss : 57.31, accuracy : 0.90\n",
            "loss : 57.28, accuracy : 0.89\n",
            "loss : 57.07, accuracy : 0.89\n",
            "loss : 57.13, accuracy : 0.89\n",
            "loss : 57.16, accuracy : 0.89\n",
            "loss : 57.14, accuracy : 0.89\n",
            "loss : 56.63, accuracy : 0.90\n",
            "loss : 56.74, accuracy : 0.89\n",
            "loss : 57.92, accuracy : 0.88\n",
            "loss : 59.24, accuracy : 0.89\n",
            "loss : 59.33, accuracy : 0.89\n",
            "loss : 59.33, accuracy : 0.89\n",
            "loss : 58.59, accuracy : 0.89\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}